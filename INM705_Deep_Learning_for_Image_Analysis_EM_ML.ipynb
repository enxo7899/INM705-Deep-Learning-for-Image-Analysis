{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "torch.Size([6])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/milan/INM705-Deep-Learning-for-Image-Analysis/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
            "  warnings.warn(\n",
            "/home/milan/INM705-Deep-Learning-for-Image-Analysis/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training FasterCNN...\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "targets should not be none when in training mode",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 177\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Train both models separately\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# print(\"\\nTraining WasteClassifier...\")\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# train.train_model(model1, optimizer1, criterion1, train_loader, val_loader)\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# print(\"\\nTraining Custom ResNet Model...\")\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# train.train_model(model2, optimizer2, criterion2, train_loader, val_loader)\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining FasterCNN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 177\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Evaluation function\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(model, test_loader):\n",
            "File \u001b[0;32m~/INM705-Deep-Learning-for-Image-Analysis/train.py:18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, criterion, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/INM705-Deep-Learning-for-Image-Analysis/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/INM705-Deep-Learning-for-Image-Analysis/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/INM705-Deep-Learning-for-Image-Analysis/venv/lib/python3.11/site-packages/torchvision/models/detection/generalized_rcnn.py:62\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtargets should not be none when in training mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets:\n",
            "File \u001b[0;32m~/INM705-Deep-Learning-for-Image-Analysis/venv/lib/python3.11/site-packages/torch/__init__.py:1559\u001b[0m, in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[0;32m-> 1559\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
            "\u001b[0;31mAssertionError\u001b[0m: targets should not be none when in training mode"
          ]
        }
      ],
      "source": [
        "# Necessary imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import detection\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "import train\n",
        "import models\n",
        "\n",
        "# Check for device: use MPS if available, otherwise use CPU\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define dataset path\n",
        "# dataset_path = '/Users/enxom/Desktop/INM705 CW/dataset1'\n",
        "dataset_path = './dataset1'\n",
        "\n",
        "# Collect data helper functions\n",
        "def is_valid_file(file_path):\n",
        "    valid_extensions = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp']\n",
        "    return any(file_path.endswith(ext) for ext in valid_extensions)\n",
        "\n",
        "def collect_data(directory):\n",
        "    data = []\n",
        "    labels = []\n",
        "    class_to_idx = {cls: idx for idx, cls in enumerate(os.listdir(directory)) if os.path.isdir(os.path.join(directory, cls))}\n",
        "    for cls, idx in class_to_idx.items():\n",
        "        class_path = os.path.join(directory, cls)\n",
        "        for file_path in glob.glob(os.path.join(class_path, '*')):\n",
        "            if is_valid_file(file_path):\n",
        "                data.append(file_path)\n",
        "                labels.append(idx)\n",
        "    return data, labels, class_to_idx\n",
        "\n",
        "\n",
        "\n",
        "# Data transformations and dataset preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), \n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Custom Dataset Class\n",
        "class CustomImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Load and split dataset\n",
        "data, labels, class_to_idx = collect_data(dataset_path)\n",
        "dataset = CustomImageDataset(data, labels, transform)\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Compute class weights for imbalanced datasets\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "torch.save(class_weights, 'checkpoints/waste.pth')\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
        "\n",
        "class_weights_dict = dict(zip(np.unique(labels), class_weights))\n",
        "def run_faster_cnn(img, model, classes):\n",
        "    img = np.array(Image.open(img))\n",
        "    img = transforms.toTensor()(img)\n",
        "    out = model([img])\n",
        "    scores = out[0]['scores']\n",
        "    boxes = out[0]['boxes']\n",
        "    classes = out[0]['classes']\n",
        "    if len(scores) > 0:\n",
        "        plt.imshow(img)\n",
        "        ax = plt.gca()\n",
        "        for box in boxes.detach().numpy():\n",
        "            xbox_min, ybox_min, xbox_max, ybox_max = box\n",
        "            width = xbox_max - xbox_min\n",
        "            height = ybox_max - ybox_min\n",
        "            rect = Rectangle((xbox_min, ybox_min), width, height, linewidth=2, edgecolour='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "        plt.show()\n",
        "    class_labels = [classes[val.item()-1] for val in classes]\n",
        "    return class_labels\n",
        "\n",
        "# faster_weights = torch.load(\"checkpoints/waste.pth\", map_location=device)\n",
        "print(class_weights_tensor.size())\n",
        "# faster_weights = class_weights_tensor\n",
        "# print(type(detection.FasterRCNN_ResNet50_FPN_Weights))\n",
        "# print(type(class_weights_tensor))\n",
        "# print(type(faster_weights))\n",
        "\n",
        "def load_model(weights_dict):\n",
        "    # frcnn_args_inference = {'box_score_thresh': 0.75, 'box_detections_per_img': 32}\n",
        "    model = detection.fasterrcnn_resnet50_fpn(weights_dict)\n",
        "    # model = detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    # model.load_state_dict(weights_dict)\n",
        "    model.eval()\n",
        "\n",
        "    # # Load the pre-trained Faster R-CNN model\n",
        "    # model = detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # # # Load your custom weights (assuming weights_tensor is your PyTorch tensor with size 6, each being 224x224)\n",
        "    # # weights_tensor = ...  # Your PyTorch tensor weights here\n",
        "\n",
        "    # # Define component names where you want to load the weights\n",
        "    # component_names = ['backbone', 'rpn', 'roi_heads']\n",
        "\n",
        "    # # Load weights into specific components of the model\n",
        "    # for name, weights in zip(component_names, weights_tensor):\n",
        "    #     component = getattr(model, name)\n",
        "    #     component.load_state_dict({'weight': weights})\n",
        "\n",
        "    # # Ensure the model is in evaluation mode after loading weights\n",
        "    # model.eval()\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "faster_cnn_model = load_model(class_weights_dict)\n",
        "faster_cnn_model._modules.keys()\n",
        "\n",
        "def resnet18(num_classes):\n",
        "    return models.ResNet(models.BasicBlock, [2, 2, 2, 2], num_classes)\n",
        "\n",
        "# Instantiate both models\n",
        "num_classes=len(class_to_idx)\n",
        "# model1 = models.WasteClassifier(num_classes).to(device)\n",
        "# model2 = models.resnet18(num_classes).to(device)\n",
        "model3 = faster_cnn_model.to(device)\n",
        "\n",
        "# Optimizers and loss functions\n",
        "# optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
        "# optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
        "optimizer3 = optim.Adam(model3.parameters(), lr=0.001)\n",
        "# criterion1 = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "# criterion2 = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "criterion3 = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "\n",
        "# Train both models separately\n",
        "# print(\"\\nTraining WasteClassifier...\")\n",
        "# train.train_model(model1, optimizer1, criterion1, train_loader, val_loader)\n",
        "\n",
        "# print(\"\\nTraining Custom ResNet Model...\")\n",
        "# train.train_model(model2, optimizer2, criterion2, train_loader, val_loader)\n",
        "\n",
        "print(\"\\nTraining FasterCNN...\")\n",
        "# train.train_model(model3, optimizer3, criterion3, train_loader, val_loader)\n",
        "# How to call it?\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    total_test = 0\n",
        "    correct_test = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (preds == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    accuracy = correct_test / total_test * 100\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate both models\n",
        "print(\"\\nEvaluating WasteClassifier...\")\n",
        "# evaluate_model(model1, test_loader)\n",
        "\n",
        "print(\"\\nEvaluating Custom ResNet Model...\")\n",
        "# evaluate_model(model2, test_loader)\n",
        "\n",
        "print(\"\\nEvaluating Faster CNN Model...\")\n",
        "evaluate_model(model3, test_loader)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
