{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'state_dict'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m compute_class_weight(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(labels), y\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     90\u001b[0m class_weights_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(labels), class_weights))\n\u001b[0;32m---> 91\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints/waste.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     92\u001b[0m class_weights_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(class_weights, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_faster_cnn\u001b[39m(img, model, classes):\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'state_dict'"
          ]
        }
      ],
      "source": [
        "# Necessary imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import detection\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "import train\n",
        "import models\n",
        "\n",
        "# Check for device: use MPS if available, otherwise use CPU\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define dataset path\n",
        "# dataset_path = '/Users/enxom/Desktop/INM705 CW/dataset1'\n",
        "dataset_path = './dataset1'\n",
        "\n",
        "# Collect data helper functions\n",
        "def is_valid_file(file_path):\n",
        "    valid_extensions = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp']\n",
        "    return any(file_path.endswith(ext) for ext in valid_extensions)\n",
        "\n",
        "def collect_data(directory):\n",
        "    data = []\n",
        "    labels = []\n",
        "    class_to_idx = {cls: idx for idx, cls in enumerate(os.listdir(directory)) if os.path.isdir(os.path.join(directory, cls))}\n",
        "    for cls, idx in class_to_idx.items():\n",
        "        class_path = os.path.join(directory, cls)\n",
        "        for file_path in glob.glob(os.path.join(class_path, '*')):\n",
        "            if is_valid_file(file_path):\n",
        "                data.append(file_path)\n",
        "                labels.append(idx)\n",
        "    return data, labels, class_to_idx\n",
        "\n",
        "\n",
        "\n",
        "# Data transformations and dataset preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), \n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Custom Dataset Class\n",
        "class CustomImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Load and split dataset\n",
        "data, labels, class_to_idx = collect_data(dataset_path)\n",
        "dataset = CustomImageDataset(data, labels, transform)\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Compute class weights for imbalanced datasets\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights_dict = dict(zip(np.unique(labels), class_weights))\n",
        "torch.save(class_weights, 'checkpoints/waste.pth')\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
        "\n",
        "def run_faster_cnn(img, model, classes):\n",
        "    img = np.array(Image.open(img))\n",
        "    img = transforms.toTensor()(img)\n",
        "    out = model([img])\n",
        "    scores = out[0]['scores']\n",
        "    boxes = out[0]['boxes']\n",
        "    classes = out[0]['classes']\n",
        "    if len(scores) > 0:\n",
        "        plt.imshow(img)\n",
        "        ax = plt.gca()\n",
        "        for box in boxes.detach().numpy():\n",
        "            xbox_min, ybox_min, xbox_max, ybox_max = box\n",
        "            width = xbox_max - xbox_min\n",
        "            height = ybox_max - ybox_min\n",
        "            rect = Rectangle((xbox_min, ybox_min), width, height, linewidth=2, edgecolour='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "        plt.show()\n",
        "    class_labels = [classes[val.item()-1] for val in classes]\n",
        "    return class_labels\n",
        "\n",
        "faster_weights = torch.load(\"checkpoints/waste.pth\", map_location=device)\n",
        "print(class_weights_tensor.size())\n",
        "# faster_weights = class_weights_tensor\n",
        "# print(type(detection.FasterRCNN_ResNet50_FPN_Weights))\n",
        "# print(type(class_weights_tensor))\n",
        "# print(type(faster_weights))\n",
        "\n",
        "def load_model(weights_dict):\n",
        "    # frcnn_args_inference = {'box_score_thresh': 0.75, 'box_detections_per_img': 32}\n",
        "    model = detection.fasterrcnn_resnet50_fpn(weights_dict)\n",
        "    # model = detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    # model.load_state_dict(weights_dict)\n",
        "    model.eval()\n",
        "\n",
        "    # # Load the pre-trained Faster R-CNN model\n",
        "    # model = detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # # # Load your custom weights (assuming weights_tensor is your PyTorch tensor with size 6, each being 224x224)\n",
        "    # # weights_tensor = ...  # Your PyTorch tensor weights here\n",
        "\n",
        "    # # Define component names where you want to load the weights\n",
        "    # component_names = ['backbone', 'rpn', 'roi_heads']\n",
        "\n",
        "    # # Load weights into specific components of the model\n",
        "    # for name, weights in zip(component_names, weights_tensor):\n",
        "    #     component = getattr(model, name)\n",
        "    #     component.load_state_dict({'weight': weights})\n",
        "\n",
        "    # # Ensure the model is in evaluation mode after loading weights\n",
        "    # model.eval()\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "faster_cnn_model = load_model(class_weights_dict)\n",
        "faster_cnn_model._modules.keys()\n",
        "\n",
        "def resnet18(num_classes):\n",
        "    return models.ResNet(models.BasicBlock, [2, 2, 2, 2], num_classes)\n",
        "\n",
        "# Instantiate both models\n",
        "num_classes=len(class_to_idx)\n",
        "# model1 = models.WasteClassifier(num_classes).to(device)\n",
        "# model2 = models.resnet18(num_classes).to(device)\n",
        "model3 = faster_cnn_model.to(device)\n",
        "\n",
        "# Optimizers and loss functions\n",
        "# optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
        "# optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
        "optimizer3 = optim.Adam(model3.parameters(), lr=0.001)\n",
        "# criterion1 = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "# criterion2 = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "criterion3 = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "\n",
        "# Train both models separately\n",
        "# print(\"\\nTraining WasteClassifier...\")\n",
        "# train.train_model(model1, optimizer1, criterion1, train_loader, val_loader)\n",
        "\n",
        "# print(\"\\nTraining Custom ResNet Model...\")\n",
        "# train.train_model(model2, optimizer2, criterion2, train_loader, val_loader)\n",
        "\n",
        "print(\"\\nTraining FasterCNN...\")\n",
        "train.train_model(model3, optimizer3, criterion3, train_loader, val_loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    total_test = 0\n",
        "    correct_test = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (preds == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    accuracy = correct_test / total_test * 100\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate both models\n",
        "print(\"\\nEvaluating WasteClassifier...\")\n",
        "# evaluate_model(model1, test_loader)\n",
        "\n",
        "print(\"\\nEvaluating Custom ResNet Model...\")\n",
        "# evaluate_model(model2, test_loader)\n",
        "\n",
        "print(\"\\nEvaluating Faster CNN Model...\")\n",
        "evaluate_model(model3, test_loader)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
